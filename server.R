
#server <- function(input,output){
shinyServer(function(input, output) {

  output$Scripts_DCR_1 <- renderText({
      paste(
      "if (!require('data.table')) { install.packages('data.table', dependencies = TRUE) ; library(data.table)} # Data manipulation",
      "if (!require('dplyr')) { install.packages('dplyr', dependencies = TRUE) ; library(dplyr)} # Date manipulation",
      "if (!require('lubridate')) { install.packages('lubridate', dependencies = TRUE) ; library(lubridate)} # Date manipulation",
      "if (!require('tidyr')) { install.packages('tidyr', dependencies = TRUE) ; library(tidyr)} # Handy utility functions",
      "if (!require('tidyverse')) { install.packages('tidyverse', dependencies = TRUE) ; library(tidyverse)} # Handy utility functions",
      "if (!require('scrubr')) { install.packages('scrubr', dependencies = TRUE) ; library(scrubr)} # Duplication",
      "if (!require('ggplot2')) { install.packages('ggplot2', dependencies = TRUE) ; library(ggplot2)} # Visualization",
      "if (!require('randomForestSRC')) { install.packages('randomForestSRC', dependencies = TRUE) ; library(randomForestSRC)} # Random Forests R-package",
      "if (!require('dismo')) { install.packages('dismo', dependencies = TRUE) ; library(dismo)} # R-Package for Boosted Regression Trees and gbmstep",
      "if (!require('stringr')) { install.packages('stringr', dependencies = TRUE) ; library(stringr)} # String manipulation",
      sep="\n")
  })

  output$Scripts_DCR_2 <- renderText({
      paste(
      "setwd('/Your_Directory/')",
      "files = list.files(pattern = '*.csv')",
      "f=1 # Your data set",
      "df <- fread(files[f]) # read your data set",
      "df <- as.data.frame(df) # convert it to dataframe", 
      sep="\n")
  })

  output$Scripts_DCR_3 <- renderText({
      paste(
      "dim(df)",
      " [1] 19735    29",
      sep="\n")
  })

  output$Scripts_DCR_4 <- renderText({
      paste(
      "df <- df[,order(colnames(df))]",
      "str(df)",
      "  'data.frame':   19735 obs. of  29 variables:",
      "  $ Appliances : chr  60 60 50 50 60 50 60 60 60 70 ...",
      "  $ date       : chr  '2016-01-11T17:00:00Z' '2016-01-11T17:10:00Z' '2016-01-11T17:20:00Z' '2016-01-11T17:30:00Z' ...",
      "  $ lights     : chr  30 30 30 40 40 40 50 50 40 40 ...",
      "  $ Press_mm_hg: num  734 734 734 734 734 ...",
      "  $ RH_1       : num  47.6 46.7 46.3 46.1 46.3 ...",
      "  $ RH_2       : num  44.8 44.7 44.6 44.6 44.5 ...",
      "  $ RH_3       : num  44.7 44.8 44.9 45 45 ...",
      "  $ RH_4       : num  45.6 46 45.9 45.7 45.5 ...",
      "  $ RH_5       : num  55.2 55.2 55.1 55.1 55.1 ...",
      "  $ RH_6       : num  84.3 84.1 83.2 83.4 84.9 ...",
      "  $ RH_7       : num  41.6 41.6 41.4 41.3 41.2 ...",
      "  $ RH_8       : num  48.9 48.9 48.7 48.6 48.6 ...",
      "  $ RH_9       : num  45.5 45.6 45.5 45.4 45.4 ...",
      "  $ RH_out     : chr  92 92 92 92 92 ...",
      "  $ rv1        : chr  13.3 18.6 28.6 45.4 10.1 ...",
      "  $ rv2        : chr  13.3 18.6 28.6 45.4 10.1 ...",
      "  $ T_out      : chr  6.6 6.48 6.37 6.25 6.13 ...",
      "  $ T1         : num  19.9 19.9 19.9 19.9 19.9 ...",
      "  $ T2         : num  19.2 19.2 19.2 19.2 19.2 ...",
      "  $ T3         : num  19.8 19.8 19.8 19.8 19.8 ...",
      "  $ T4         : num  19 19 18.9 18.9 18.9 ...",
      "  $ T5         : num  17.2 17.2 17.2 17.2 17.2 ...",
      "  $ T6         : chr  7.03 6.83 6.56 6.43 6.37 ...",
      "  $ T7         : num  17.2 17.2 17.2 17.1 17.2 ...",
      "  $ T8         : num  18.2 18.2 18.2 18.1 18.1 18.1 18.1 18.1 18.1 18.1 ...",
      "  $ T9         : num  17 17.1 17 17 17 ...",
      "  $ Tdewpoint  : chr  5.3 5.2 5.1 5 4.9 ...",
      "  $ Visibility : chr  63 59.2 55.3 51.5 47.7 ...",
      "  $ Windspeed  : chr  7 6.67 6.33 6 5.67 ...",
      sep="\n")
  })

  output$Scripts_DCR_5 <- renderText({
      paste(
      "df$date <- ymd_hms(df$date)",
      "df$Appliances <- as.numeric(df$Appliances)",
      "df$lights <- as.numeric(df$lights)",
      "df$T6 <- as.numeric(df$T6)",
      "df$RH_6 <- as.numeric(df$RH_6)",
      "df$T_out <- as.numeric(df$T_out)",
      "df$RH_out <- as.numeric(df$RH_out)",
      "df$Windspeed <- as.numeric(df$Windspeed)",
      "df$Visibility <- as.numeric(df$Visibility)",
      "df$Tdewpoint <- as.numeric(df$Tdewpoint)",
      "df$rv1 <- as.numeric(df$rv1)",
      "df$rv2 <- as.numeric(df$rv2)",       
      sep="\n")
  })

  output$Scripts_DCR_6 <- renderText({
      paste(
      "str(df)",
      "  'data.frame':   19735 obs. of  29 variables:",
      "  $ Appliances : int  60 60 50 50 60 50 60 60 60 70 ...",
      "  $ date       : POSIXct, format  '2016-01-11T17:00:00Z' '2016-01-11T17:10:00Z' '2016-01-11T17:20:00Z' '2016-01-11T17:30:00Z' ...",
      "  $ lights     : int  30 30 30 40 40 40 50 50 40 40 ...",
      "  $ Press_mm_hg: num  734 734 734 734 734 ...",
      "  $ RH_1       : num  47.6 46.7 46.3 46.1 46.3 ...",
      "  $ RH_2       : num  44.8 44.7 44.6 44.6 44.5 ...",
      "  $ RH_3       : num  44.7 44.8 44.9 45 45 ...",
      "  $ RH_4       : num  45.6 46 45.9 45.7 45.5 ...",
      "  $ RH_5       : num  55.2 55.2 55.1 55.1 55.1 ...",
      "  $ RH_6       : num  84.3 84.1 83.2 83.4 84.9 ...",
      "  $ RH_7       : num  41.6 41.6 41.4 41.3 41.2 ...",
      "  $ RH_8       : num  48.9 48.9 48.7 48.6 48.6 ...",
      "  $ RH_9       : num  45.5 45.6 45.5 45.4 45.4 ...",
      "  $ RH_out     : num  92 92 92 92 92 ...",
      "  $ rv1        : num  13.3 18.6 28.6 45.4 10.1 ...",
      "  $ rv2        : num  13.3 18.6 28.6 45.4 10.1 ...",
      "  $ T_out      : num  6.6 6.48 6.37 6.25 6.13 ...",
      "  $ T1         : num  19.9 19.9 19.9 19.9 19.9 ...",
      "  $ T2         : num  19.2 19.2 19.2 19.2 19.2 ...",
      "  $ T3         : num  19.8 19.8 19.8 19.8 19.8 ...",
      "  $ T4         : num  19 19 18.9 18.9 18.9 ...",
      "  $ T5         : num  17.2 17.2 17.2 17.2 17.2 ...",
      "  $ T6         : num  7.03 6.83 6.56 6.43 6.37 ...",
      "  $ T7         : num  17.2 17.2 17.2 17.1 17.2 ...",
      "  $ T8         : num  18.2 18.2 18.2 18.1 18.1 18.1 18.1 18.1 18.1 18.1 ...",
      "  $ T9         : num  17 17.1 17 17 17 ...",
      "  $ Tdewpoint  : num  5.3 5.2 5.1 5 4.9 ...",
      "  $ Visibility : num  63 59.2 55.3 51.5 47.7 ...",
      "  $ Windspeed  : num  7 6.67 6.33 6 5.67 ...",
      sep="\n")
  })

  output$Scripts_DCR_7 <- renderText({
      paste(
      "cat(sprintf(' Number of missing data in the dataframe is: %s\\n', sum(is.na(df))),",
      "    sprintf('Percentage of missing data is : %s\\n', (sum(is.na(df))*100)/nrow(df)),",
      "    sprintf('Number of NULLs in the dataframe is: %s\\n', sum(is.null(df))))",
      " Number of missing data in the dataframe is: 0",
      " Percentage of missing data is : 0",
      " Number of NULLs in the dataframe is: 0",
      "",
      "colSums(is.na(df))",
      " Appliances        date      lights Press_mm_hg        RH_1        RH_2",
      "          0           0           0           0           0           0",
      "       RH_3        RH_4        RH_5        RH_6        RH_7        RH_8",
      "          0           0           0           0           0           0",
      "       RH_9      RH_out         rv1         rv2       T_out          T1",
      "          0           0           0           0           0           0",
      "         T2          T3          T4          T5          T6          T7",
      "          0           0           0           0           0           0",
      "         T8          T9   Tdewpoint  Visibility   Windspeed",
      "          0           0           0           0           0",
      "",
      "cat(sprintf(' Number of duplicated rows is: %s\\n', duplicated(df) %>% sum()))",
      " Number of duplicated rows is: 0",
      "",
      "summary(df)",
      "   Appliances           date                         lights        Press_mm_hg         RH_1            RH_2            RH_3    ",
      " Min.   :  10.00   Min.   :2016-01-11 17:00:00   Min.   : 0.000   Min.   :729.3   Min.   :27.02   Min.   :20.46   Min.   :28.77",
      " 1st Qu.:  50.00   1st Qu.:2016-02-14 23:15:00   1st Qu.: 0.000   1st Qu.:750.9   1st Qu.:37.33   1st Qu.:37.90   1st Qu.:36.90",
      " Median :  60.00   Median :2016-03-20 05:30:00   Median : 0.000   Median :756.1   Median :39.66   Median :40.50   Median :38.53",
      " Mean   :  97.69   Mean   :2016-03-20 05:30:00   Mean   : 3.802   Mean   :755.5   Mean   :40.26   Mean   :40.42   Mean   :39.24",
      " 3rd Qu.: 100.00   3rd Qu.:2016-04-23 11:45:00   3rd Qu.: 0.000   3rd Qu.:760.9   3rd Qu.:43.07   3rd Qu.:43.26   3rd Qu.:41.76",
      " Max.   :1080.00   Max.   :2016-05-27 18:00:00   Max.   :70.000   Max.   :772.3  Max.   :63.36   Max.   :56.03   Max.   :50.16 ",
      "",
      "      RH_4            RH_5            RH_6            RH_7            RH_8            RH_9           RH_out            rv1      ",
      " Min.   :27.66   Min.   :29.82   Min.   : 1.00   Min.   :23.20    Min.   :29.60   Min.   :29.17   Min.   : 24.00   Min.   : 0.01",
      " 1st Qu.:35.53   1st Qu.:45.40   1st Qu.:30.02   1st Qu.:31.50    1st Qu.:39.07   1st Qu.:38.50   1st Qu.: 70.33   1st Qu.:12.49",
      " Median :38.40   Median :49.09   Median :55.29   Median :34.86    Median :42.38   Median :40.90   Median : 83.67   Median :24.89",
      " Mean   :39.03   Mean   :50.95   Mean   :54.61   Mean   :35.39    Mean   :42.94   Mean   :41.55   Mean   : 79.75   Mean   :24.98",
      " 3rd Qu.:42.16   3rd Qu.:53.66   3rd Qu.:83.23   3rd Qu.:39.00    3rd Qu.:46.54   3rd Qu.:44.34   3rd Qu.: 91.67   3rd Qu.:37.58",
      " Max.   :51.09   Max.   :96.32   Max.   :99.90   Max.   :51.40    Max.   :58.78   Max.   :53.33   Max.   :100.00   Max.   :49.99",
      "",
      "      rv2               T_out              T1              T2              T3              T4              T5              T6   ",
      " Min.   : 0.01   Min.   :-5.00   Min.   :16.79   Min.   :16.10    Min.   :17.20   Min.   :15.10   Min.   :15.33   Min.   :-6.065",
      " 1st Qu.:12.49   1st Qu.: 3.66   1st Qu.:20.76   1st Qu.:18.79    1st Qu.:20.79   1st Qu.:19.53   1st Qu.:18.28   1st Qu.: 3.627",
      " Median :24.89   Median : 6.91   Median :21.60   Median :20.00    Median :22.10   Median :20.67   Median :19.39   Median : 7.300",
      " Mean   :24.98   Mean   : 7.41   Mean   :21.69   Mean   :20.34    Mean   :22.27   Mean   :20.86   Mean   :19.59   Mean   : 7.911",
      " 3rd Qu.:37.58   3rd Qu.:10.40   3rd Qu.:22.60   3rd Qu.:21.50    3rd Qu.:23.29   3rd Qu.:22.10   3rd Qu.:20.62   3rd Qu.:11.256",
      " Max.   :49.99   Max.   :26.10   Max.   :26.26   Max.   :29.86    Max.   :29.24   Max.   :26.20   Max.   :25.80   Max.   :28.290",
      "",      
      "       T7              T8              T9          Tdewpoint        Visibility      Windspeed   ",
      " Min.   :15.39   Min.   :16.31   Min.   :14.89   Min.   :-6.600   Min.   : 1.00   Min.   : 0.000",
      " 1st Qu.:18.70   1st Qu.:20.79   1st Qu.:18.00   1st Qu.: 0.900   1st Qu.:29.00   1st Qu.: 2.000",
      " Median :20.03   Median :22.10   Median :19.39   Median : 3.433   Median :40.00   Median : 3.667",
      " Mean   :20.27   Mean   :22.03   Mean   :19.49   Mean   : 3.761   Mean   :38.33   Mean   : 4.040",
      " 3rd Qu.:21.60   3rd Qu.:23.39   3rd Qu.:20.60   3rd Qu.: 6.567   3rd Qu.:40.00   3rd Qu.: 5.500",
      " Max.   :26.00   Max.   :27.23   Max.   :24.50   Max.   :15.500   Max.   :66.00   Max.   :14.000",
      sep="\n")
  })

  output$Scripts_DCR_8 <- renderText({
      paste(  
      "cor_df <- round(cor(df[3:29]),2)",
      "",
      "# Get lower triangle of the correlation matrix",
      "get_lower_tri<-function(cormat){",
      "    cormat[upper.tri(cormat)] <- NA",
      "    return(cormat)",
      "}",
      "",
      "# Create a ggheatmap",
      "ggplot(cor_df_melted, aes(Var2, Var1, fill = value))+",
      "    geom_tile(color = 'white')+",
      "    geom_text(aes(Var2, Var1, label = value), color = 'black', size = 2) +",
      "    scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white',",
      "    midpoint = 0, limit = c(-1,1), space = 'Lab',",
      "    name='Pearson\nCorrelation') +",
      "    theme(panel.background = element_rect(fill = 'white', color = NA),",
      "        panel.border = element_rect(fill = NA, color = 'gray40'),",
      "        axis.text.x = element_text(angle=90,hjust=1,vjust=.5),",
      "        axis.text.y = element_text(),",
      "        axis.title = element_blank(),",
      "        plot.title = element_text(hjust = 0.5),",
      "        legend.position = c(.9, .2), legend.text = element_text(hjust = 0.5),",
      "        legend.title = element_text(hjust = 0.5),",
      "        legend.key.size = unit(1.5,'line')) +",
      "    coord_fixed()",
      sep="\n")
  })
  # here put the heatmap

  output$Scripts_DCR_9 <- renderText({
      paste(
      "df_temp <- cbind(df[c(2,7,8,17,23,20,22,24,26)])",
      "df_temp <- reshape2::melt(df_temp, id = 'date') # melt the data to bring it to a format that can be visualized using ggplot2",
      "",
      "ggplot(df_temp, aes(x = date, y = value, group = variable, color = variable)) +",
      "    geom_line(alpha = .5) +",
      "    scale_color_manual('legend', values = c('T6' = 'darkviolet',",
      "                                            'T_out' = 'red',",
      "                                            'T3' = 'steelblue4',",
      "                                            'T5' = 'steelblue3',",
      "                                            'T7' = 'steelblue2',",
      "                                            'T9' = 'dodgerblue',",
      "                                            'RH_3' = 'gold4',",
      "                                            'RH_4' = 'gold3')) +",
      "    theme_bw()",
      sep="\n")
  })
  # here put the correlation plot

  output$Scripts_DCR_10 <- renderText({
      paste(
      "rf <- rfsrc(Appliances ~ .,",
      "                      data = df[c(1,3:29)],",
      "                      tree.err = TRUE, ntree = 100, importance = T)",
      "",
      "# Assess relative influence of feature parameters",
      "Rel_imp <- as.data.frame(sort(rf$importance, decreasing = TRUE))",
      "Rel_imp[,2] <- rownames(Rel_imp)",
      "rownames(Rel_imp) = NULL",
      "colnames(Rel_imp) <- c('importance', 'Variable')",
      "",
      "# calculate relative influences in %",
      "Rel_imp$importance_p <- round((Rel_imp$importance*100)/sum(Rel_imp$importance), digits = 2)",
      "Rel_imp$sort <- 1:27",
      "x <- ggplot(data=Rel_imp, aes(x=reorder(Variable, -sort), y=importance_p)) +",
      "       geom_bar(stat='identity', position=position_dodge()) +",
      "       coord_flip() +",
      "       xlab('Feature parameters') +",
      "       ylab('Relative influences [%]') +",
      "       theme(panel.background = element_rect(fill = 'white', color = NA),",
      "           panel.border = element_rect(fill = NA, color = 'gray40'),",
      "           panel.grid.major = element_line(color = 'grey93'),",
      "           panel.grid.minor = element_line(color = 'grey98'),",
      "           legend.position='none')",
      "",
      "# In this figure, we can see most of those features are co-correlated and show a very similar trend and a high variety of values ",
      sep="\n")
  })

  output$Scripts_DCR_11 <- renderText({
      paste(
      "df[c('T_out', 'T7', 'RH_4', 'rv1', 'rv2', 'Lights', 'Visibility')] = NULL",
      sep="\n")
  })

  output$Scripts_DCR_12 <- renderText({
      paste(
        "# Melt the data to bring it in a format that is suitable for our ggplot",
        "df_melted <- reshape2::melt(df2, id = c('date','Appliances'))",
        "",
        "ggplot(df_melted, aes(value)) + ",
        "  geom_histogram(aes(y=..density..), fill = 'steelblue') +",
        "  geom_density(alpha=.1, fill='white', color = 'steelblue4') +",
        "  facet_wrap(variable ~ ., scales = 'free')",
      sep="\n")
  })

  output$Scripts_DCR_13 <- renderText({
      paste(
      "df$TimeOfDay <- hour(df$date)",
      "df$Weekday <- wday(df$date)",
      "df$Julian_day <- yday(df$date)",
      "",
      "# rescaling feature parameters",
      "df2 <- scale(df[c(1,3:27)])",
      "df2 <- cbind(df[2],df2)",
      "",
      "# Save the cleaned dataframe in your directory",
      "fwrite(df2, '/Your_directory/KAG_energy_data_cleaned_scaled.csv')",
      sep="\n")
  })

###########################################################################################################################

  output$Scripts_DCPy_1 <- renderText({
    paste(
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "from sklearn.ensemble import RandomForestRegressor",
    "import time",
    sep="\n")
  })

  output$Scripts_DCPy_2 <- renderText({
    paste(
    "df = pd.read_csv('/Your_directory/KAG_energy_data.csv')",
    "",
    "# Convert date column todatetime format:",
    "df['date'] =  pd.to_datetime(df['date'])",
    "",
    "# order columns alphabetically:",
    "df = df.reindex(sorted(df.columns), axis=1)",
    "df.info()",
    "  <class 'pandas.core.frame.DataFrame'>",
    "  RangeIndex: 19735 entries, 0 to 19734",
    "  Data columns (total 29 columns):",
    "  date           19735 non-null datetime64[ns]",
    "  Appliances     19735 non-null int64",
    "  Press_mm_hg    19735 non-null float64",
    "  RH_1           19735 non-null float64",
    "  RH_2           19735 non-null float64",
    "  RH_3           19735 non-null float64",
    "  RH_4           19735 non-null float64",
    "  RH_5           19735 non-null float64",
    "  RH_6           19735 non-null float64",
    "  RH_7           19735 non-null float64",
    "  RH_8           19735 non-null float64",
    "  RH_9           19735 non-null float64",
    "  RH_out         19735 non-null float64",
    "  T1             19735 non-null float64",
    "  T2             19735 non-null float64",
    "  T3             19735 non-null float64",
    "  T4             19735 non-null float64",
    "  T5             19735 non-null float64",
    "  T6             19735 non-null float64",
    "  T7             19735 non-null float64",
    "  T8             19735 non-null float64",
    "  T9             19735 non-null float64",
    "  T_out          19735 non-null float64",
    "  Tdewpoint      19735 non-null float64",
    "  Visibility     19735 non-null float64",
    "  Windspeed      19735 non-null float64",
    "  lights         19735 non-null int64",
    "  rv1            19735 non-null float64",
    "  rv2            19735 non-null float64",
    "  dtypes: datetime64[ns](1), float64(26), int64(2)",
    "  memory usage: 4.4 MB",
    sep="\n")
  })

  output$Scripts_DCPy_3 <- renderText({
    paste(
    "#Number of nulls or NAs in aeachll column",
    "df.isna().sum().sort_values(ascending = True)",
    "df.isnull().sum().sort_values(ascending = True)",
    "  date           0",
    "  Appliances     0",
    "  Press_mm_hg    0",
    "  RH_1           0",
    "  RH_2           0",
    "  RH_3           0",
    "  RH_4           0",
    "  RH_5           0",
    "  RH_6           0",
    "  RH_7           0",
    "  RH_8           0",
    "  RH_9           0",
    "  RH_out         0",
    "  T1             0",
    "  T2             0",
    "  T3             0",
    "  T4             0",
    "  T5             0",
    "  T6             0",
    "  T7             0",
    "  T8             0",
    "  T9             0",
    "  Tout           0",
    "  Tdewpoint      0",
    "  Visibility     0",
    "  Windspeed      0",
    "  lights         0",
    "  rv1            0",
    "  rv2            0",
    sep="\n")
  })

  output$Scripts_DCPy_4 <- renderText({
    paste(
    "# Use feature parameters to see the correlation",
    "corr = df.corr()",
    "",
    "# Mask the repeated values (upper triangle)",
    "mask = np.triu(corr)",
    "",
    "fig, ax = plt.subplots(figsize=(20,12)) # Sample figsize in inches",
    "sns.heatmap(corr, annot = True, mask=mask, fmt='.2f');",
    sep="\n")
  })

  output$Scripts_DCPy_5 <- renderText({
    paste(
    "df2 = df[['date', 'T6', 'T_out', 'T3', 'T5', 'T7', 'T9', 'RH_3', 'RH_4']]",
    "fig, ax = plt.subplots(figsize=(20,8)) # Sample figsize in inches",
    sep="\n")
  })

  output$Scripts_DCPy_6 <- renderText({
    paste(
    "# evaluete relative influences ...",
    "dataset = df.values",
    "x = dataset[:, 1:28]",
    "y = dataset[:, 0]",
    "rfs = RandomForestRegressor(n_estimators=100)",
    "model = rfs.fit(x,y)",
    "",
    "# Calculate feature importances",
    "importances = model.feature_importances_",
    "",
    "# Sort feature importances in descending order",
    "indices = np.argsort(importances)[::-1]",
    "",
    "# Rearrange feature names so they match the sorted feature importances",
    "cols = df.columns[1:28]",
    "names = [cols[i] for i in indices]",
    "",
    "# make the plot",
    "fig, ax = plt.subplots(figsize=(20,8)) # Sample figsize in inches",
    "",
    "# Barplot: Add bars",
    "plt.bar(range(x.shape[1]), importances[indices])",
    "",
    "# Add feature names as x-axis labels",
    "plt.xticks(range(x.shape[1]), names, rotation=90, fontsize = 16)",
    "plt.yticks(fontsize = 16)",
    "plt.xlabel('Feature parameters', fontsize=16)",
    "plt.ylabel('Relative importances', fontsize=16)",
    "",
    "# Show plot",
    "plt.show()",
    sep="\n")
  })

  output$Scripts_DCPy_7 <- renderText({
    paste(
    "df = df.drop(['T_out','T7','RH_4','rv1','rv2','Lights','Visibility'], axis = 1)",
    sep="\n")
  })

  output$Scripts_DCPy_8 <- renderText({
    paste(
    "# Histogram of all the features to understand the distribution",
    "df.hist(density=True, bins = 30, histtype='stepfilled', color='steelblue', edgecolor='none', figsize= (20,14))",
    "plt.ylabel('Probability')",
    "plt.xlabel('Variables');",
    sep="\n")
  })

  output$Scripts_DCPy_9 <- renderText({
    paste(
    "df['TimeOfDay'] = df['date'].dt.hour",
    "df['Weekday'] = df['date'].dt.dayofweek",
    "df['Julian_day'] = df['date'].dt.strftime('%j')",
    sep="\n")
  })

###########################################################################################################################

  output$code_AllAlgorithmsR_1 <- renderText({
    paste( 
    "if (!require('data.table')) { install.packages('data.table', dependencies = TRUE) ; library(data.table)}", 
    "if (!require('randomForestSRC')) { install.packages('randomForestSRC', dependencies = TRUE) ; library(randomForestSRC)}", 
    "if (!require('dismo')) { install.packages('dismo', dependencies = TRUE) ; library(dismo)}", 
    "if (!require('FNN')) { install.packages('FNN', dependencies = TRUE) ; library(FNN)}", 
    "if (!require('e1071')) { install.packages('e1071', dependencies = TRUE) ; library(e1071)}", 
    "if (!require('ranger')) { install.packages('ranger', dependencies = TRUE) ; library(ranger)}", 
    "if (!require('ANN2')) { install.packages('ANN2', dependencies = TRUE) ; library(ANN2)}", 
    "if (!require('randomForestSRC')) { install.packages('randomForestSRC', dependencies = TRUE) ; library(randomForestSRC)}", 
    "if (!require('randomForestSRC')) { install.packages('randomForestSRC', dependencies = TRUE) ; library(randomForestSRC)}", 
    sep="\n")
  })

  output$code_AllAlgorithmsR_2 <- renderText({
    paste(
    "# Load data",
    "setwd('/Your_directory/')",
    "files = list.files(pattern = '*.csv')",
    "",
    "f=1 # KAG_energy_data_cleaned_scaled.csv",
    "df <- fread(files[f])",
    "df <- as.data.frame(df)",
    "df$date <- ymd_hms(df$date)",
    "df[2] = NULL # Remove date column",
    "",
    "",
    "",
    sep="\n")
  })

  output$code_AllAlgorithmsR_3 <- renderText({
    paste(
    "splits <- 1:10",
    "",
    "for(s in 1:length(splits)) {",
    "   cat('Running set', s, '\\n')",
    "",
    "   # Split dataframe randomly into 75% train and 25% test sets",
    "   random_split <- sample(c(rep(TRUE,ceiling(nrow(df)*0.75)),rep(FALSE,floor(nrow(df)*0.25))))",
    "   df_train <- df[random_split, ]",
    "   df_test  <- df[!random_split, ]",
    "",
    "   # Prepare matrix of train and test sets for some models",
    "   x_matrix = as.matrix(df_train[3:25])",
    "   y_matrix = as.matrix(df_train[1])",
    "   test = as.matrix(df_test[3:25])",
    "",
    "   ### Fit the the six algorithms:",
    "   # Fit Random Forests (RF)",
    "   rf <- randomForestSRC::rfsrc(Appliances ~ ., data = df_train[,c(1,3:25)], nodesize = 5, ",
    "                 na.action = 'na.impute', tree.err = TRUE, ntree = 200, importance = TRUE)",
    "   # rf_2 <- randomForest::randomForest(Appliances ~ ., data = df_train[,c(2,3:25)], # option 2",
    "                   ntree = 200, importance = T, proximity = T)",
    "   rf_preds <- predict.rfsrc(rf, newdata = df_test[,c(1,3:25)], na.action = 'na.impute')",
    "",
    "   # Fit Boosted regression trees (BRT)",
    "   brt <- dismo::gbm.step(data=df_train, gbm.x = c(3:25), gbm.y = 1, ",
    "                   family = 'gaussian', bag.fraction=0.5, learning.rate=0.001, max.trees = 10001)",
    "   brt_preds = dismo::predict(brt, newdata = df_test[,c(3:25)], n.trees=brt$gbm.call$best.trees)",
    "   brt_preds = as.data.frame(brt_preds)",
    "   colnames(brt_preds) = 'predictions'",
    "",
    "   # Fit K nearest neighbours (KNN)",
    "   knn_preds <- FNN::knn.reg(x_matrix, test=as.matrix(df_test[3:25]), y = y_matrix, k = 5, algorithm = c('kd_tree', 'cover_tree', 'brute'))",
    "   knn_preds = as.data.frame(knn_preds$pred)",
    "   colnames(knn_preds) = 'predictions'",
    "",
    "   # Fit Support Vector Regression (SVR)",
    "   svr = e1071::svm(Appliances ~ ., data = df_train[c(1,3:25)], type = 'eps-regression')",
    "   svr_preds <- predict(svr, df_test[3:25])",
    "   svr_preds = as.data.frame(svr_preds)",
    "   colnames(svr_preds) = 'predictions'",
    "",
    "   # Fit  Extra Trees Regression (ET)",
    "   et <- ranger::ranger(Appliances ~ .,",
    "                   data = df_train[c(1,3:25)], splitrule = 'extratrees', num.trees = 200)",
    "   et_preds <- predict(et, df_test[3:25])",
    "   et_preds = as.data.frame(et_preds$predictions)",
    "   colnames(et_preds) = 'predictions'",
    "",
    "   # Fit Artificial Neural Networks (ANN)",
    "   ann <- ANN2::neuralnetwork(x_matrix , y = y_matrix,",
    "                               activ.functions = 'relu', hidden.layers = 5,",
    "                               n.epochs = 200, batch.size = 16, verbose = TRUE,",
    "                               optim.type = 'adam', loss.type = 'log',  learn.rates = .00003)",
    "   ann_preds <- predict(ann, newdata = test)",
    "   ann_preds <- as.data.frame(ann_preds$predictions)",
    "   ann_preds$predictions <- as.numeric(as.character(ann_preds$'ann_preds$predictions'))",
    "   colnames(ann_preds)[2] = 'predictions'",
    "",
    "   ### model evaluation ",
    "   observed = as.data.frame(df_test[2])",
    "",
    "   # Adjusted-R-squared value",
    "   rf_AdjRsqr <- summary(lm(rf_preds$predicted ~ rf_preds$yvar))$adj.r.squared",
    "   brt_AdjRsqr <- summary(lm(brt_preds$predictions ~ observed$Appliances))$adj.r.squared",
    "   knn_AdjRsqr <- summary(lm(knn_preds$predictions ~ observed$Appliances))$adj.r.squared",
    "   svr_AdjRsqr <- summary(lm(svr_preds$predictions ~ observed$Appliances))$adj.r.squared",
    "   et_AdjRsqr <- summary(lm(et_preds$predictions ~ observed$Appliances))$adj.r.squared",
    "   ann_AdjRsqr <- summary(lm(ann_preds$predictions ~ observed$Appliances))$adj.r.squared",
    "",
    "   # Mean absolute error",
    "   rf_MeanAE <- mean(abs(rf_preds$predicted - rf_preds$yvar))",
    "   brt_MeanAE <- mean(abs(brt_preds$predictions - observed$Appliances))",
    "   knn_MeanAE <- mean(abs(knn_preds$predictions - observed$Appliances))",
    "   svr_MeanAE <- mean(abs(svr_preds$predictions - observed$Appliances))",
    "   et_MeanAE <- mean(abs(et_preds$predictions - observed$Appliances))",
    "   ann_MeanAE <- mean(abs(ann_preds$predictions - observed$Appliances))",
    "",
    "   # Mediaan absolute error",
    "   rf_MedeanAE <- median(abs(rf_preds$predicted - rf_preds$yvar))",
    "   brt_MedeanAE <- median(abs(brt_preds$predictions - observed$Appliances))",
    "   knn_MedeanAE <- median(abs(knn_preds$predictions - observed$Appliances))",
    "   svr_MedeanAE <- median(abs(svr_preds$predictions - observed$Appliances))",
    "   et_MedeanAE <- median(abs(et_preds$predictions - observed$Appliances))",
    "   ann_MedeanAE <- median(abs(ann_preds$predictions - observed$Appliances))",
    "",
    "   # Root mean square error",
    "   rf_RMSE <- sqrt(mean((rf_preds$predicted - rf_preds$yvar)^2))",
    "   brt_RMSE <- sqrt(mean((brt_preds$predictions - observed$Appliances)^2))",
    "   knn_RMSE <- sqrt(mean((knn_preds$predictions - observed$Appliances)^2))",
    "   svr_RMSE <- sqrt(mean((svr_preds$predictions - observed$Appliances)^2))",
    "   et_RMSE <- sqrt(mean((et_preds$predictions - observed$Appliances)^2))",
    "   ann_RMSE <- sqrt(mean((ann_preds$predictions - observed$Appliances)^2))",
    "",
    "   # Pearson correlation coefficient",
    "   rf_pearson <- cor(as.data.frame(cbind(rf_preds$yvar, rf_preds$predicted)), method = 'pearson')[1,2]",
    "   brt_pearson <- cor(as.data.frame(cbind(observed$Appliances, brt_preds$predictions)), method = 'pearson')[1,2]",
    "   knn_pearson <- cor(as.data.frame(cbind(observed$Appliances, knn_preds$predictions)), method = 'pearson')[1,2]",
    "   svr_pearson <- cor(as.data.frame(cbind(observed$Appliances, svr_preds$predictions)), method = 'pearson')[1,2]",
    "   et_pearson <- cor(as.data.frame(cbind(observed$Appliances, et_preds$predictions)), method = 'pearson')[1,2]",
    "   ann_pearson <- cor(as.data.frame(cbind(observed$Appliances, ann_preds$predictions)), method = 'pearson')[1,2]",
    "",
    "   # Compile all data on one single table",
    "   rf_result <- as.data.frame(cbind(rf_AdjRsqr, rf_MeanAE, rf_MedeanAE, rf_RMSE, rf_pearson))",
    "   rf_result$model = 'RF'",
    "   colnames(rf_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "   brt_result <- as.data.frame(cbind(brt_AdjRsqr, brt_MeanAE, brt_MedeanAE, brt_RMSE, brt_pearson))",
    "   brt_result$model = 'BRT'",
    "   colnames(brt_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "   knn_result <- as.data.frame(cbind(knn_AdjRsqr, knn_MeanAE, knn_MedeanAE, knn_RMSE, knn_pearson))",
    "   knn_result$model = 'KNN'",
    "   colnames(knn_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "   svr_result <- as.data.frame(cbind(svr_AdjRsqr, svr_MeanAE, svr_MedeanAE, svr_RMSE, svr_pearson))",
    "   svr_result$model = 'SVR'",
    "   colnames(svr_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "   et_result <- as.data.frame(cbind(et_AdjRsqr, et_MeanAE, et_MedeanAE, et_RMSE, et_pearson))",
    "   et_result$model = 'ET'",
    "   colnames(et_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "   ann_result <- as.data.frame(cbind(ann_AdjRsqr, ann_MeanAE, ann_MedeanAE, ann_RMSE, ann_pearson))",
    "   ann_result$model = 'ANN'",
    "   colnames(ann_result) <- c('R2_score', 'MeanAE', 'MedeanAE', 'RMSE', 'Correlation_coefficient', 'Model')",
    "",
    "   result <- rbind(rf_result, brt_result, knn_result, svr_result, et_result, ann_result)",
    "   result$Split = paste0('Split_', s)",
    "   result <- result[c(7, 6, 1:5)] ",
    "",
    "   if(s==1){",
    "       results = result",
    "   } else {",
    "       results <- rbind(results, result)",
    "   }",
    "   fwrite(results, '/Your_directory/R_all_algorithms_evaluations.csv')",
    "}",
    sep="\n")
  })

###########################################################################################################################

  output$code_AllAlgorithmsPy_1 <- renderText({
    paste(
    "import tensorflow",
    "import keras",
    "import numpy as np",
    "import pandas as pd",
    "import os",
    "import time",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor",
    "from sklearn.neural_network import MLPRegressor",
    "from sklearn import neighbors",
    "from sklearn.svm import SVR",
    "from keras.models import Sequential",
    "from keras.layers import Dense",
    "from keras.optimizers import Adam",
    "from sklearn.linear_model import LinearRegression",
    "from sklearn.preprocessing import LabelEncoder",
    "from sklearn import metrics",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error",
    "from math import sqrt",
    "from math import sqrt",
    "from scipy.stats import pearsonr",
    "from keras.models import load_model",
    sep="\n")
  })

  output$code_AllAlgorithmsPy_2 <- renderText({
    paste(
    "df = pd.read_csv('/media/kakouei/data/KaranHub/Kaggle/Appliances_energy_prediction/KAG_energy_data_cleaned_scaled.csv')",
    "df = df.drop('date', axis = 1)",
    "df.columns",
    "",
### add printed columns here
    sep="\n")
  })

  output$code_AllAlgorithmsPy_3 <- renderText({
    paste(
    "dataset = df.values",
    "dataset.shape",
    "",
### add printed columns here
    "# Define the X (feature parameters) and y (response variable, appliances):",
    "X = dataset[:, 2:24]",
    "Y = dataset[:, 0]",
    sep="\n")
  })

  output$code_AllAlgorithmsPy_4 <- renderText({
    paste(
    "# define the number of splits (Which is 10)",
    "splits = np.arange(1,11,1).tolist()",
    "",
    "for split in splits:",
    "",
    "   # Split the data randomly into 75% train and 25% test data sets:",
    "   train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.25)",
    "",
    "   ### build the models' structure:",
    "   # Fit Random Forests (RF)",
    "   rfs = RandomForestRegressor(n_estimators = 200, random_state = 22, bootstrap=True)",
    "   rfs.fit(train_X, train_y)",
    "",
    "   # Fit Boosted regression trees (BRT)",
    "   brt = GradientBoostingRegressor(loss='ls', n_estimators=200, validation_fraction=0.1)",
    "   brt.fit(train_X,train_y)",
    "",
    "   # Fit K nearest neighbours (KNN)",
    "   knn = neighbors.KNeighborsRegressor(n_neighbors = 5, algorithm='auto')",
    "   knn.fit(train_X,train_y)",
    "",
    "   # Fit Support Vector Regression (SVR)",
    "   svr = SVR(kernel='rbf')",
    "   svr.fit(train_X,train_y)",
    "   # Fit  Extra Trees Regression (ET)",
    "   et = ExtraTreesRegressor(n_estimators=200,  criterion='mse')",
    "   et.fit(train_X,train_y)",
    "",
    "   # Fit Artificial Neural Networks (ANN)",
    "   training_variables = train_X.shape[1]",
    "   ann = Sequential()",
    "   ann.add(Dense(32, input_shape=(training_variables,), activation='relu'))",
    "   ann.add(Dense(64, activation='relu'))",
    "   ann.add(Dense(128, activation='relu'))",
    "   ann.add(Dense(256, activation='relu'))",
    "   ann.add(Dense(512, activation='relu'))",
    "   ann.add(Dense(1))",
    "   adam = Adam(lr=0.00003)",
    "   ann.compile(loss='mean_squared_error', optimizer=adam)",
    "   ann.fit(train_X, train_y, epochs=200, batch_size=16, verbose=0)",
    "",
    "   ### Model evaluation ",
    "   # Adjusted-R-squared value",
    "   rfs_r2 = metrics.r2_score(test_y, rfs.predict(test_X))",
    "   brt_r2 = metrics.r2_score(test_y, brt.predict(test_X))",
    "   knn_r2 = metrics.r2_score(test_y, knn.predict(test_X))",
    "   svr_r2 = metrics.r2_score(test_y, svr.predict(test_X))",
    "   et_r2 = metrics.r2_score(test_y, et.predict(test_X))",
    "   ann_r2 = metrics.r2_score(test_y, ann.predict(test_X))",
    "",
    "   # Mean absolute error",
    "   rfs_mae = metrics.mean_absolute_error(test_y, rfs.predict(test_X))",
    "   brt_mae = metrics.mean_absolute_error(test_y, brt.predict(test_X))",
    "",
    "   knn_mae = metrics.mean_absolute_error(test_y, knn.predict(test_X))",
    "   svr_mae = metrics.mean_absolute_error(test_y, svr.predict(test_X))",
    "   et_mae = metrics.mean_absolute_error(test_y, et.predict(test_X))",
    "   ann_mae = metrics.mean_absolute_error(test_y, ann.predict(test_X))",
    "",
    "   # Mediaan absolute error",
    "   rfs_medae = metrics.median_absolute_error(test_y, rfs.predict(test_X))" ,
    "   brt_medae = metrics.median_absolute_error(test_y, brt.predict(test_X))",
    "   knn_medae = metrics.median_absolute_error(test_y, knn.predict(test_X))",
    "   svr_medae = metrics.median_absolute_error(test_y, svr.predict(test_X))",
    "   et_medae = metrics.median_absolute_error(test_y, et.predict(test_X))",
    "   ann_medae = metrics.median_absolute_error(test_y, ann.predict(test_X))",
    "",
    "   # Root mean square error",
    "   rfs_rmse = sqrt(metrics.mean_squared_error(test_y, rfs.predict(test_X)))",
    "   brt_rmse = sqrt(metrics.mean_squared_error(test_y, brt.predict(test_X)))",
    "   knn_rmse = sqrt(metrics.mean_squared_error(test_y, knn.predict(test_X)))",
    "   svr_rmse = sqrt(metrics.mean_squared_error(test_y, svr.predict(test_X)))",
    "   et_rmse = sqrt(metrics.mean_squared_error(test_y, et.predict(test_X)))",
    "   ann_rmse = sqrt(metrics.mean_squared_error(test_y, ann.predict(test_X)))",
    "",
    "   # Pearson correlation coefficient",
    "   rfs_pr2 = pearsonr(test_y, rfs.predict(test_X))[0]",
    "   brt_pr2 = pearsonr(test_y, brt.predict(test_X))[0]",
    "   knn_pr2 = pearsonr(test_y, knn.predict(test_X))[0]",
    "   svr_pr2 = pearsonr(test_y, svr.predict(test_X))[0]",
    "   et_pr2 = pearsonr(test_y, et.predict(test_X))[0]",
    "   a = pd.DataFrame(test_y)",
    "   b = pd.DataFrame(ann.predict(test_X))",
    "   ab = pd.concat([a, b], axis=1)",
    "   ab.columns = ['a', 'b']",
    "   col_1 = ab['a']",
    "   col_2 = ab['b']",
    "   ann_pr2 = col_1.corr(col_2)",
    "",
    "   sp = str(splits)",
    "",
    "   ### save results on a table",
    "   result = []",
    "   if split == 1:",
    "       result = {'Split': [sp, sp, sp, sp, sp, sp],",
    "                 'Model': ['RF', 'BRT', 'KNN', 'SVR', 'ET', 'ANN'],",
    "                 'R2_score': [rfs_r2, brt_r2, knn_r2, svr_r2, et_r2, ann_r2],",
    "                 'MeanAE': [rfs_mae, brt_mae, knn_mae, svr_mae, et_mae, ann_mae],",
    "                 'MedeanAE': [rfs_medae, brt_medae, knn_medae, svr_medae, et_medae, ann_medae],",
    "                 'RMSE': [rfs_rmse, brt_rmse, knn_rmse, svr_rmse, et_rmse, ann_rmse],",
    "                 'Correlation_coefficient': [rfs_pr2, brt_pr2, knn_pr2, svr_pr2, et_pr2, ann_pr2]}",
    "       result = pd.DataFrame(result)",
    "       result['Split'] = result['Split'].apply(lambda x: f'Split_{x}')",
    "       results = result",
    "   else:",
    "       result = {'Split': [sp, sp, sp, sp, sp, sp],",
    "                 'Model': ['RF', 'BRT', 'KNN', 'SVR', 'ET', 'ANN'],",
    "                 'R2_score': [rfs_r2, brt_r2, knn_r2, svr_r2, et_r2, ann_r2],",
    "                 'MeanAE': [rfs_mae, brt_mae, knn_mae, svr_mae, et_mae, ann_mae],",
    "                 'MedeanAE': [rfs_medae, brt_medae, knn_medae, svr_medae, et_medae, ann_medae],",
    "                 'RMSE': [rfs_rmse, brt_rmse, knn_rmse, svr_rmse, et_rmse, ann_rmse],",
    "                 'Correlation_coefficient': [rfs_pr2, brt_pr2, knn_pr2, svr_pr2, et_pr2, ann_pr2]}",
    "       result = pd.DataFrame(result)",
    "       result['Split'] = result['Split'].apply(lambda x: f'Split_{x}')",
    "       results = results.append(result)",
    "",
    "   name = '_'.join(['/media/kakouei/data/KaranHub/Kaggle/Appliances_energy_prediction/Models/Py',",
    "                   'all_algorithms_evaluations.csv'])",
    "",
    "   results.to_csv(name,index = None, header=True)",
    sep="\n")
  })

###########################################################################################################################

  output$code_ANNsPy_1 <- renderText({
    paste( 
    "import tensorflow",
    "import keras",
    "import os",
    "import numpy as np",
    "import pandas as pd",
    "from sklearn.preprocessing import LabelEncoder",
    "from sklearn.pipeline import Pipeline",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.linear_model import LinearRegression",
    "from keras.models import Sequential",
    "from keras.layers import Dense",
    "from keras.optimizers import Adam",
    "from keras.callbacks import TensorBoard, ModelCheckpoint",
    "import matplotlib.pyplot as plt",
    "from keras.models import load_model",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error",
    "from math import sqrt",
    sep="\n")
  })


  output$code_ANNsPy_2 <- renderText({
    paste( 
    ">>> df = pd.read_csv('/Your_directory/KAG_energy_data_cleaned_scaled.csv')",
    "", 
    "# Drop the date column", 
    "df = df.drop('date', axis = 1)",
    "",
    "# Define your response (Y) and feature (X) parameters' column",
    "X = dataset[:, 2:26]",
    "Y = dataset[:,0]",
    sep="\n")
  })

  output$code_ANNsPy_3 <- renderText({
    paste( 
    "import random",
    "random.seed(42)",
    "",
    "# Define the number of cross-validations as splits",
    "splits = np.arange(1, 11, 1).tolist()",
    "",
    "# Define the number of dense layers of your neural networks. Adjust this according to the size of the data set (e.g., number of feature parameters, etc.)",
    "dense_layers = [1, 3, 5, 7]",
    "",
    "# Define the size of neurons in the hidden layers of your neural networks. Adjust this according to the size of the data set (e.g., number of feature parameters, etc.)",
    "layer_sizes = [128, 256, 512, 1024, 4096]",
    "",
    "for split in splits:",
    "",
    "    # Split the  data into 75% training and 25% test and validation sets",
    "    X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.25)",
    "",
    "    # Split the 20% test and validation sets into 10% test and 10% validation sets",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)",
    "",
    "    # You can print the shape of train, test and validation sets forthe response variable and features feature parameters",
    "    print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)",
    "",
    "    training_variables = X_train.shape[1]",
    "",
    "    # start the loop for dense layers that are defined above",
    "    for dense_layer in dense_layers:",
    "       # start the loop for layer sizes that are defined above",
    "       for layer_size in layer_sizes:",
    "",
    "            # set model name",
    "            model_name = f'{layer_size}-neurons-{dense_layer}-hidden_layers'",
    "",
    "            # set model numbers to be appended to each model name. As 200 models (epochs) will be saved, we need to write an if else condition to have three digits for all model numbers",
    "            if layer_size < 100:",
    "                model_checkpoint = '0' + model_name + '_{epoch:03d}.hdf5'",
    "            else:",
    "                model_checkpoint = model_name + '_{epoch:03d}.hdf5'",
    "",
    "            # Set or make directory for model output folder for each of the 10-fold cross validations",
    "            os.chdir('/'.join(['','Your_directory','Splits',str(split)]))",
    "            model_save_path = os.path.join(os.getcwd(), 'model')",
    "            if not os.path.exists(model_save_path):",
    "                os.mkdir(model_save_path)",
    "",
    "            # Print model name to be aware of loop progress while it is running",
    "            print(model_name)",
    "",
    "            # Save only best models and allow for online tracking by tensorboard",
    "            checkpoint = ModelCheckpoint(filepath=os.path.join(model_save_path, model_checkpoint), verbose=0, save_best_only=True, monitor = 'val_loss')",
    "",
    "            # Provide a tensorboard for online tracking of the loss for the test and validation sets. Further details may be find in the next step below",
    "            tensorboard = TensorBoard(os.path.join(os.getcwd(), f'training_logs/{model_name}'))",
    "",
    "            # create the model",
    "            model = Sequential()",
    "",
    "            # create the first dense layer with 32 neurons. Adjust this based on the size of the data set (e.g., number of feature parameters)",
    "            model.add(Dense(32, input_shape=(training_variables,), activation='relu'))",
    "",
    "            # Run a loop over the above-defined hidden layers and their sizes",
    "            for l in range(dense_layer-1):",
    "                model.add(Dense(layer_size, activation='relu'))",
    "",
    "            model.add(Dense(1))",
    "",
    "            # Define the learning rate",
    "            adam = Adam(lr=0.00003)",
    "",
    "            # Compile the keras model, using the MSE as your evaluation metric",
    "            model.compile(loss='mean_squared_error', optimizer=adam)",
    "",
    "            # Fit the ANN model. Adjust the epochs and batch size according to the data set",
    "            model.fit(X_train, Y_train," ,
    "                      epochs=200," ,
    "                      batch_size=32,",
    "                      verbose=0,",
    "                      validation_data=(X_val, Y_val),",
    "                      callbacks=[checkpoint, tensorboard])",
    "",
    "            # evaluate the model",
    "            predictions = model.predict(X_test)",
    "",
    "            R2_score = r2_score(Y_test,predictions)",
    "            MedAE = median_absolute_error(Y_test,predictions)",
    "            MeanAE = mean_absolute_error(Y_test,predictions)",
    "            RMSE = sqrt(mean_squared_error(Y_test, predictions))",
    "            a = pd.DataFrame(Y_test)",
    "            b = pd.DataFrame(predictions)",
    "            ab = pd.concat([a, b], axis=1)",
    "            ab.columns = ['a', 'b']",
    "            col_1 = ab['a']",
    "            col_2 = ab['b']",
    "            pr2 = col_1.corr(col_2)",
    "",
    "            if (layer_size == 128 and dense_layer == 5):",
    "                result = {'Model': model_name,",
    "                'R2_score': [R2_score],",
    "                'MeanAE': [MeanAE],",
    "                'MedianAE': [MedAE],",
    "                'RMSE': [RMSE],",
    "                'Correlation_coefficient': [pr2]}",
    "                results = pd.DataFrame(result)",
    "            else:",
    "                result = {'Model': model_name,",
    "                'R2_score': [R2_score],",
    "                'MeanAE': [MeanAE],",
    "                'MedianAE': [MedAE],",
    "                'RMSE': [RMSE],",
    "                'Correlation_coefficient': [pr2]}",
    "                results = pd.DataFrame(result)",
    "                results = results.append(result)",
    "",
    "            results.to_csv('/media/kakouei/data/KaranHub/Kaggle/Appliances_energy_prediction/Models/ANNsPy_v1_Model_performances.csv' ,index = None, header=True)",                  
    sep="\n")
  })

  output$code_ANNsPy_4 <- renderText({
    paste( 
    "# Open the terminal and go to the directory that models are being saved",
    "user§YourPC: cd /Your_Directory/Splits/1/",
    "# The folder should contain two folders of 'model' and 'training logs'. Check this by typing 'ls' in the terminal",
    "",
    "# Then type:",
    "tensorboard --logdir=training_logs",
    "# After printing this command, a link will be appeared. Click on the link or copy-paste it into your browser to follow model performances",
    sep="\n")
  })
  # Here add the loss screenshots

############################################

  output$code_ANNsPy_6 <- renderText({
    paste( 
    "# Save the results into your directory",
    ">>> results.to_csv('/Your_Directory/ANNsPy_Model_performances.csv',index = None, header=True)",
    sep="\n")
  })

###########################################################################################################################

}
